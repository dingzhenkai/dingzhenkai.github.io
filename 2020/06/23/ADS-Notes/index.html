<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
   
  <meta name="keywords" content="丁星乐、学习、工作、生活" />
   
  <meta name="description" content="分享丁星乐的学习、工作与享乐时光" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
    ADS_Notes |  丁星乐
  </title>
  <meta name="generator" content="hexo-theme-yilia-plus">
  
  <link rel="shortcut icon" href="/favicon.ico" />
  
  
<link rel="stylesheet" href="/css/style.css">

  
<script src="/js/pace.min.js"></script>


  

  

</head>

</html>

<body>
  <div id="app">
    <main class="content">
      <section class="outer">
  <article id="post-ADS-Notes" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  ADS_Notes
</h1>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/06/23/ADS-Notes/" class="article-date">
  <time datetime="2020-06-23T05:45:55.000Z" itemprop="datePublished">2020-06-23</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/">课程笔记</a>
  </div>

      
      
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> 字数统计:</span>
            <span class="post-count">15k字</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> 阅读时长≈</span>
            <span class="post-count">56分钟</span>
        </span>
    </span>
</div>

      
    </div>
    

    
    
    <div class="tocbot"></div>





    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <p>一些知识点，概念，问题以及解决方案</p>
<h2 id="Let-1"><a href="#Let-1" class="headerlink" title="Let 1"></a>Let 1</h2><h3 id="为什么cpu频率不能一直增加："><a href="#为什么cpu频率不能一直增加：" class="headerlink" title="为什么cpu频率不能一直增加："></a>为什么cpu频率不能一直增加：</h3><ul>
<li>频率越高，耗能越好，热量越多，不好散热</li>
<li>信号传播限制。cpu大概1cm长，信号从一头传到另一头也需要时间，所以不能无限变快</li>
</ul>
<h3 id="什么是NUMA："><a href="#什么是NUMA：" class="headerlink" title="什么是NUMA："></a>什么是NUMA：</h3><p>随着科学计算、事务处理对计算机性能要求的不断提高，SMP（对称多处理器）系统的应用越来越广泛，规模也越来越大，但由于传统的SMP系统中，所有处理器都共享系统总线，因此当处理器的数目增大时，系统总线的竞争冲突加大，系统总线将成为瓶颈，所以目前SMP系统的CPU数目一般只有数十个，可扩展能力受到极大限制。NUMA技术有效结合了SMP系统易编程性和MPP（大规模并行）系统易扩展性的特点，较好解决了SMP系统的可扩展性问题，已成为当今高性能服务器的主流体系结构之一。</p>
<p><strong>非统一内存访问架构</strong>（英语：<strong>Non-uniform memory access</strong>，简称NUMA）是一种为<a href="https://zh.wikipedia.org/wiki/多處理器" target="_blank" rel="noopener">多处理器</a>的电脑设计的内存架构，内存访问时间取决于内存相对于处理器的位置。在NUMA下，处理器访问它自己的本地内存的速度比非本地内存（内存位于另一个处理器，或者是处理器之间共享的内存）快一些。</p>
<p><a href="https://zhuanlan.zhihu.com/p/33621500" target="_blank" rel="noopener">深挖NUMA</a></p>
<h3 id="并行与并发的区别"><a href="#并行与并发的区别" class="headerlink" title="并行与并发的区别"></a>并行与并发的区别</h3><p><strong>并行(parallel)</strong>：指在同一时刻，有多条指令在多个处理器上同时执行。就好像两个人各拿一把铁锨在挖坑，一小时后，每人一个大坑。所以无论从微观还是从宏观来看，二者都是一起执行的。</p>
<p><img src="https://segmentfault.com/img/bV1GiB?w=449&h=192" alt="图片描述"><br><strong>并发(concurrency)</strong>：指在同一时刻只能有一条指令执行，但多个进程指令被快速的轮换执行，使得在宏观上具有多个进程同时执行的效果，但在微观上并不是同时执行的，只是把时间分成若干段，使多个进程快速交替的执行。这就好像两个人用同一把铁锨，轮流挖坑，一小时后，两个人各挖一个小一点的坑，要想挖两个大一点得坑，一定会用两个小时。</p>
<p><img src="https://segmentfault.com/img/bV1GiJ?w=449&h=192" alt="图片描述"></p>
<h3 id="给并行与分布式系统分类"><a href="#给并行与分布式系统分类" class="headerlink" title="给并行与分布式系统分类"></a>给并行与分布式系统分类</h3><p>SISD</p>
<p><img src="https://pic2.zhimg.com/80/v2-195580bb521e48584bdb7564fdab7c8d_1440w.png" alt="img"></p>
<p><strong>单指令流单数据流</strong>（<a href="https://zh.wikipedia.org/wiki/英文" target="_blank" rel="noopener">英文</a>：Single instruction, single data，<a href="https://zh.wikipedia.org/wiki/縮寫" target="_blank" rel="noopener">缩写</a>：<strong>SISD</strong>），每个指令部件每次仅译码一条指令，而且在执行时仅为操作部件提供一份数据。符合<a href="https://zh.wikipedia.org/wiki/冯·诺伊曼结构" target="_blank" rel="noopener">冯·诺伊曼结构</a>。</p>
<p>单指令流单数据流是<a href="https://zh.wikipedia.org/wiki/費林分類法" target="_blank" rel="noopener">费林分类法</a>中4种计算机处理架构类别的一种。在这个分类系统中，分类根据是<a href="https://zh.wikipedia.org/wiki/指令" target="_blank" rel="noopener">指令</a>流和<a href="https://zh.wikipedia.org/wiki/資料" target="_blank" rel="noopener">资料</a>流的数量，以此根据来划分计算机处理架构的类别。根据<a href="https://zh.wikipedia.org/w/index.php?title=米高·J·費林&action=edit&redlink=1" target="_blank" rel="noopener">米高·J·费林</a>的观点，当指令、资料处理流水化/管线化时，单指令流单数据流也可以拥有<a href="https://zh.wikipedia.org/wiki/並行計算" target="_blank" rel="noopener">并行计算</a>的特点。<a href="https://zh.wikipedia.org/wiki/指令管線化" target="_blank" rel="noopener">管线化的指令读取执行</a>在当代的单指令流单数据流处理机种上很常见。<a href="https://zh.wikipedia.org/wiki/單指令流單數據流#cite_note-1" target="_blank" rel="noopener">[1]</a><a href="https://zh.wikipedia.org/wiki/單指令流單數據流#cite_note-2" target="_blank" rel="noopener">[2]</a></p>
<p>SIMD</p>
<p><strong>单指令流多数据流</strong>（英语：<strong>Single Instruction Multiple Data</strong>，<a href="https://zh.wikipedia.org/wiki/縮寫" target="_blank" rel="noopener">缩写</a>：<strong>SIMD</strong>）是一种采用一个<a href="https://zh.wikipedia.org/wiki/控制器" target="_blank" rel="noopener">控制器</a>来控制多个<a href="https://zh.wikipedia.org/wiki/处理器" target="_blank" rel="noopener">处理器</a>，同时对一组数据（又称“<a href="https://zh.wikipedia.org/w/index.php?title=数据向量&action=edit&redlink=1" target="_blank" rel="noopener">数据向量</a>”）中的每一个分别执行<strong>相同</strong>的操作从而实现空间上的<a href="https://zh.wikipedia.org/wiki/并行" target="_blank" rel="noopener">并行</a>性的技术。</p>
<p>在<a href="https://zh.wikipedia.org/wiki/微处理器" target="_blank" rel="noopener">微处理器</a>中，单指令流多数据流技术则是一个<a href="https://zh.wikipedia.org/wiki/控制器" target="_blank" rel="noopener">控制器</a>控制多个平行的<a href="https://zh.wikipedia.org/w/index.php?title=处理微元&action=edit&redlink=1" target="_blank" rel="noopener">处理微元</a>，例如<a href="https://zh.wikipedia.org/wiki/Intel" target="_blank" rel="noopener">Intel</a>的<a href="https://zh.wikipedia.org/wiki/MMX" target="_blank" rel="noopener">MMX</a>或<a href="https://zh.wikipedia.org/wiki/SSE" target="_blank" rel="noopener">SSE</a>，以及<a href="https://zh.wikipedia.org/wiki/AMD" target="_blank" rel="noopener">AMD</a>的<a href="https://zh.wikipedia.org/wiki/3D_Now!" target="_blank" rel="noopener">3D Now!</a>指令集。</p>
<p><a href="https://zh.wikipedia.org/wiki/圖形處理器" target="_blank" rel="noopener">图形处理器</a>（GPU）拥有强大的并发处理能力和可编程流水线，面对单指令流多数据流时，运算能力远超传统CPU。<a href="https://zh.wikipedia.org/wiki/OpenCL" target="_blank" rel="noopener">OpenCL</a>和<a href="https://zh.wikipedia.org/wiki/CUDA" target="_blank" rel="noopener">CUDA</a>分别是目前最广泛使用的开源和专利<a href="https://zh.wikipedia.org/wiki/通用圖形處理器" target="_blank" rel="noopener">通用图形处理器</a>（GPGPU）运算语言。</p>
<p><img src="https://pic4.zhimg.com/v2-9c786a3bf86c115a7b786a3d1f03e24c_1440w.jpg" alt="SIMD简介"></p>
<p><img src="https://pic4.zhimg.com/80/v2-0c56ad7326f03b91fbf48bd0ce5f03ef_1440w.jpg" alt="img">标量运算与SIMD运算对比</p>
<p>如上图所示，使用标量运算一次只能对一对数据执行乘法操作，而采用SIMD乘法指令，则一次可以对四对数据同时执行乘法操作。</p>
<p>MISD</p>
<p><img src="https://pic3.zhimg.com/80/v2-e99d2bf9b745bd5255df7f14cd065ae6_1440w.png" alt="img"></p>
<p>MISD是采用多个指令流来处理单个数据流。由于实际情况中，采用多指令流处理多数据流才是更有效的方法，因此MISD只是作为理论模型出现，没有投入到实际应用之中。</p>
<p>MIMD</p>
<p>多指令流多数据流机器（MIMD）</p>
<p><img src="https://pic3.zhimg.com/80/v2-c57263ddd437016913f8916ec1b3e042_1440w.png" alt="img"></p>
<p>MIMD机器可以同时执行多个指令流，这些指令流分别对不同数据流进行操作。最新的多核计算平台就属于MIMD的范畴，例如Intel和AMD的双核处理器等都属于MIMD。</p>
<h3 id="Cache写机制，write-through-与-write-back"><a href="#Cache写机制，write-through-与-write-back" class="headerlink" title="Cache写机制，write through 与 write back"></a>Cache写机制，write through 与 write back</h3><p>Write-through（直写模式）在数据更新时，同时写入缓存Cache和后端存储。此模式的优点是操作简单；缺点是因为数据修改需要同时写入存储，数据写入速度较慢。</p>
<p>Write-back（回写模式）在数据更新时只写入缓存Cache。只在数据被替换出缓存时，被修改的缓存数据才会被写到后端存储。此模式的优点是数据写入速度快，因为不需要写存储；缺点是一旦更新后的数据未被写入存储时出现系统掉电的情况，数据将无法找回。</p>
<h3 id="snoopy-cache"><a href="#snoopy-cache" class="headerlink" title="snoopy cache"></a>snoopy cache</h3><p>In order to prevent this and maintain <a href="https://en.wikipedia.org/wiki/Cache_coherence" target="_blank" rel="noopener">cache coherence</a>, snoopy caches monitor (‘snoop on’) the memory bus to detect any writes to values that they are holding, including changes coming from other processors or distributed computers.</p>
<p>However, this approach can only work in computer architectures like <a href="https://en.wikipedia.org/wiki/SGI_Challenge" target="_blank" rel="noopener">SGI Challenge</a> and <a href="https://en.wikipedia.org/wiki/SGI_Onyx" target="_blank" rel="noopener">SGI Onyx</a> where a single memory bus is shared between all processors.</p>
<h3 id="CPU-三大架构-SMP-NUMA-MPP"><a href="#CPU-三大架构-SMP-NUMA-MPP" class="headerlink" title="CPU 三大架构 SMP NUMA MPP"></a>CPU 三大架构 SMP NUMA MPP</h3><h4 id="smp"><a href="#smp" class="headerlink" title="smp"></a>smp</h4><p>SMP （Symmetric Multiprocessing） , 对称多处理器. 顾名思义, 在SMP中所有的处理器都是对等的, 它们通过总线连接共享同一块物理内存，这也就导致了系统中所有资源(CPU、内存、I/O等)都是共享的，当我们打开服务器的背板盖，如果发现有多个cpu的槽位，但是却连接到同一个内存插槽的位置，那一般就是smp架构的服务器，日常中常见的pc啊，笔记本啊，手机还有一些老的服务器都是这个架构，其架构简单，但是拓展性能非常差，从linux 上也能看到:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls /sys/devices/system/node/<span class="comment"># 如果只看到一个node0 那就是smp架构</span></span><br></pre></td></tr></table></figure>

<p>可以看到只有仅仅一个node，经过大神们的测试发现，2至4个CPU比较适合smp架构。</p>
<h4 id="NUMA"><a href="#NUMA" class="headerlink" title="NUMA"></a>NUMA</h4><p>NUMA （ Non-Uniform Memory Access），非均匀访问存储模型，这种模型的是为了解决smp扩容性很差而提出的技术方案，如果说smp 相当于多个cpu 连接一个内存池导致请求经常发生冲突的话，numa 就是将cpu的资源分开，以node 为单位进行切割，每个node 里有着独有的core ，memory 等资源，这也就导致了cpu在性能使用上的提升，但是同样存在问题就是2个node 之间的资源交互非常慢，当cpu增多的情况下，性能提升的幅度并不是很高。所以可以看到很多明明有很多core的服务器却只有2个node区。</p>
<p><img src="http://abcdxyzk.github.io/images/kernel/2015-06-02-13.png" alt="img"></p>
<h4 id="MPP"><a href="#MPP" class="headerlink" title="MPP"></a>MPP</h4><p>MPP (Massive Parallel Processing) ，这个其实可以理解为刀片服务器，每个刀扇里的都是一台独立的smp架构服务器，且每个刀扇之间均有高性能的网络设备进行交互，保证了smp服务器之间的数据传输性能。相比numa 来说更适合大规模的计算，唯一不足的是，当其中的smp 节点增多的情况下，与之对应的计算管理系统也需要相对应的提高。</p>
<h2 id="Let-2-consistency"><a href="#Let-2-consistency" class="headerlink" title="Let 2 consistency"></a>Let 2 consistency</h2><p><strong>一致性</strong>:定义了更新出现顺序和可见性的规则,伴随着tradeoff.<br><strong>在分布式中困难原因</strong>:数据复制(cache)、并发(无共享锁)、failure(机器或网络)<br>. <strong>Strict consistency</strong>: 读到最新写的数据；同个CPU所有操作的timestamp即为执行顺序. <strong>Sequentialconsistency</strong>：最接近Strict consistency,只是将total order代替timestamp.<br><strong>SC要求</strong>：单个处理器内部操作有顺序；对同一个内存地址的访问遵循FIFO.<br><strong>IVY</strong>：<strong>MOTIVATION</strong>: 原始DSM无法保障SC；shared memory便于写并行程序,避免使用message passing.<br><strong>Centralizedmanager</strong>: read/write operation order.<br><strong>SC缺点</strong>：必须有全局控制机制；必须有内存同步操作,造成伪共享.<br><strong>RC</strong>:引入lock(同步变量), acquire() 和release() 是SC,可产生和SC相同结果.<br><strong>SC&amp;RC区别</strong>：SC中,看到相同读写顺序；RC中,以放锁顺序看到写结果.<br><strong>同步机制</strong>：update-base(eager RC),放锁时对all发modification,在拿锁的时候不需要同步操作; invalidated-based(lazy RC), 放锁时发invalidations,需要时取,可避免不必要的msg发送,可能有较大的access miss开销.<br><strong>write protoco</strong>l: 修改之前创建一个twin,放锁时通过比较产生diff.允许multiple write,消除了伪共享；节省网络发送开销.<br><strong>LRC</strong>：满足happened-before关系即可,order是应用在process interval(即一个拿放锁)上的,SC是在每个opt；<br><strong>happened-before interva</strong>l: 若间隔i在j之前,则i内所有操作在j之前<br><strong>Lamport timestamps</strong>:用来在分布式系统中决定事件顺序. 1.事件开始之前,process的计数器先+1；2.发msg时,将计数器也发送；3.接受者先将自己的计数器设置为发送方和自己计数器的最大值,再接受msg.<br><strong>SC&amp;RC缺点</strong>：慢,每次操作之前都要询问master/lock server;对参与者available要求高,要求所有成员都在.</p>
<h2 id="Let-3-eventual-consistency"><a href="#Let-3-eventual-consistency" class="headerlink" title="Let 3 eventual consistency"></a>Let 3 eventual consistency</h2><p><strong>EC:</strong> 保证最终状态convergence,选择性提供因果性保证.<br><strong>EC&amp;SC</strong>:写冲突较少；先update,再考虑是否可以serialized.<br><strong>Bayou</strong>:实现了availability(A), 低延时(L),容忍partition(P),causal+性.<br><strong>Anomalies</strong>:<strong>write-write conflict</strong>à保证状态最终convergence,即将write作为update<br>function,利用update ID&lt;time T,node ID&gt;来决定不同节点的更新顺序（X &lt; Y iff [(X.T &lt;<br>Y.T) or(X.T=Y.T and X.ID &lt; Y.ID)]）.总之,先更新,若有不一致,则rollback,同步update<br>set,replay更新函数；<br><strong>stale read</strong>à即不同node本地时钟不一致,则根据相应的updateID做出的操作顺序可能出错,采用lamport logical clock来保证因果关系；<br><strong>Tentative to show result</strong>àde-centralized:如果发现每个node的时间戳都大于N,则N之前的update都是稳定的；但是,如果有一个node离线,则没有update可以stable了.centralized:有一个server是primary,对update分配CSN（Cmt-Seq-No）,分配了的update即为stable的；但是,CSN大多数情况和tentative顺序匹配,因为server按逻辑顺序发,但是不是总是匹配的,primary可能先看到较新的update,进而分配小的CSN.总之还是倾向于central.<br><strong>COPS</strong>:在Bayou基础上,实现了可扩展(S),因为Bayou基于时间戳,不scalable.<br><strong>Causality的三种来源</strong>：Thread-of-Execution（同一个线程内部opt顺序）、Get-from（op2读到op1写的值）、Transitivity.<br><strong>Causal+</strong>：因果性+冲突处理.<br><strong>MOTIVATION</strong>: 之前系统实现causal+是Log-exchange based,限制了可扩展性,没有cross-site的因果保证.<br><strong>KEY to Scalability</strong>:metadata的依赖关系隐含了因果性,用分布式的verification代替单个的serialization,即当replication的依赖关系都满足时,才CMT.<br><strong>接口</strong>：getà无区别；put_after(key,value,deps)àclient library存储数据依赖关系deps,向site发送数据副本时附带deps,当site确保deps都被满足之后才CMT.</p>
<h2 id="let-4-crash-recovering"><a href="#let-4-crash-recovering" class="headerlink" title="let 4 crash recovering"></a>let 4 crash recovering</h2><p>(focus单机failure)一个tx内部的原子性：DO-REDO-UNDO协议.<br><strong>解决failure</strong>：Straw Manàcopy on write,但是当多个trans共享文件时,无法保障logging.<br><strong>SYSR</strong>：<strong>Log结构</strong>：append-only,减少随机访问；反向链表,后面log指向前一个log位置,保证只有append操作.<br><strong>Log规则</strong>：Write Ahead Log (WAL)协议；trans在CMT的时候,在log上append一个cmt记录<br><strong>Recovery</strong>:从后向前扫log,abort掉没有CMT标记的log,从前到后REDO已经CMT的log.<br><strong>Checkpoint**</strong>原因<strong>：避免abort掉长的trans；避免从空白状态开始recover.<br>**How to CKPT</strong>: 1.当前无action进行, 2.在log里写一个CKPT记录,包含正在进行的所有trans及指针指向其最近的log,方便undo,3.保存所有文件,dirty cacheàdisk, 4.原子地用新CKPT代替旧CKPT.<br><strong>Log类型</strong>：一般REDO&amp;UNDO都支持,UNDOà长的trans可能被abort,而REDOà对于最终CMT的长trans,CKPT可能只记录了一部分；REDO-only和UNDO-only形式也可以.<br><strong>FSD</strong>：<strong>logging File System</strong>:因为同步写很慢,有对同一个位置的冗余写,恢复需要扫整个FS；Write-back Cache,即只写回cache,cache满了之后flush,可以batch降低I/O,但是由于刷回disk顺序不确定,在crash时爆炸,甚至无法保证metadata的原子性；因此FS采用REDO-only<br>log+CKPT,只针对metadata(短trans).<br><strong>WAL+CKPT+REDO</strong>:<strong>写操作</strong>：在cache中修改,appendREDO log.<br><strong>When flush dirty data</strong>: CMT log到disk之后.<br><strong>Log Structure</strong>: 环状的disk文件,通过同步写来append新log；内存中的log是group<br>cmt到disk中的,牺牲了一定的数据稳定性,降低了I/O；通过做CKPT,定期clean.<br><strong>Recovery</strong>:从CKPT开始扫log,找到CMT的；从CKPT开始replay</p>
<h2 id="let-5-concurrency-control"><a href="#let-5-concurrency-control" class="headerlink" title="let 5 concurrency control"></a>let 5 concurrency control</h2><p>多个tx的可串行性. <strong>Serializability</strong>:<br>等价于某种串行执行；等价即为操作相同,处理冲突方法相同；冲突即为多个操作同时访问相同数据,至少有一个为写. <strong>2PL(lock-based)</strong>:</p>
<ol>
<li><p>Growing:tx拿锁,整个tx时期. </p>
</li>
<li><p>2.Shrinking:放锁, cmt期间.</p>
</li>
</ol>
<p><strong>死锁</strong>:1.顺序拿锁;开销大; 2.timeout检测,abort自己;要有redo/undo log支持.</p>
<p><strong>缺点</strong>:1.需要分布式死锁检测;2.对大量只读的场景不友好（全是只读就不需要锁）可能block更新.</p>
<p><strong>异常</strong>:p1.脏读,读到最终uncmt的中间数据.p2.不可重复读,数据读之后,被修改或者删除,then cmt.p3.幻读.读数据之后,符合条件的数据被增加. **Isolation</p>
<p>  Level(IL)<strong>: Read uncommitted(p123); Read committed(p23); Repeatable<br>  read(p3); Anomaly serialable. **Snapshot<br>  Isolation(SI)</strong>:<strong>背景</strong>: 原来依据异常划分IL,含混不清,粒度粗;<strong>思想</strong>:无锁;每个数据有多个version;消除p12;适用于大量只读tx. <strong>流程</strong>: 1.开始前有时间戳T.sts;2.读满足x(i).cts&lt;=T.sts的最新x(i);3.buffer写,添加数据到T.wset中;4.预分配cmt时间戳T.cts,确认T.wset内数据无更新,则cmt;若T.sts&lt; x.cts&lt;T.cts,则abort. (即解决w-w冲突)<strong>优点</strong>:无锁开销小;只读tx不会阻塞;<strong>缺点</strong>:不是serialable</p>
<p><strong>2PL</strong>：在2PL协议下，每个transaction都会经过两个阶段：在第一个阶段里，transaction根据需要不断地获取锁，叫做 <strong>growing phase (expanding phase)</strong>；在第二个阶段里，transaction开始释放其持有的锁，根据2PL的规则，这个transaction不能再获得新的锁，所以它所持有的锁逐渐减少，叫做 <strong>shrinking phase (contracting phase)</strong>。</p>
<p><strong>Snapshot Isolation</strong>：</p>
<p>事务在启动时得到一个数据库的版本号。事务结束时，成功提交仅当它修改的快照的数据项此时没有被外界改变，即没有<a href="https://zh.wikipedia.org/w/index.php?title=%E5%86%99-%E5%86%99%E5%86%B2%E7%AA%81&action=edit&redlink=1" target="_blank" rel="noopener">写-写冲突</a>，否则事务流产（abort）。</p>
<h4 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h4><p>文章主要是抨击了原有的基于异常（anomaly）来定义隔离级别（isolation level）的这种方法，原有的划分方法说明是含混不清的，粒度也是不够细的。作者分析了当年流行的isolation levels（后面缩写IL）后，提出了一种新的mutiversionisolation——snapshot isolation。ps.论文比较老（SIGMOD95），有些名词比较生僻。</p>
<h4 id="ANSI-IL和三种异常"><a href="#ANSI-IL和三种异常" class="headerlink" title="ANSI IL和三种异常"></a>ANSI IL和三种异常</h4><p><strong>P1: Dirty Read</strong>：T1改了某个数据项，T2在T1 commit之前读到，然而T1并没有commit，所以T2觉得自己一定读了一个假的数据项</p>
<p><strong>P2: Unrepeatable**</strong>（<strong><strong>Fuzzy</strong></strong>）<strong>**Read</strong>：同样是T2，读完一个数据后，这个数据被T1修改或者删除了，然后T1commit了，然而T2再想读取该数据，却不能够了。</p>
<p><strong>P3:Phantome**</strong>（幽灵）**：T1根绝某种查找策略先读到一些数据，T2开始执行，产生一些符合该搜索策略的data，T1再次执行时，发现与第一次查找的数据不一致。</p>
<p>这三种异常<strong>都不会在**</strong>serial**的情况下出现。</p>
<p><strong>为什么说定义含混？</strong></p>
<p>拿定义P1来说，这一句话可以理解为以下两种情况：（abort or commit 1表示 a1 or c1）</p>
<ol>
<li><p>w1[x]… …r2[x]… … (a1 and c2 in either order),</p>
</li>
<li><p>w1[x]… …r2[x]… … ((c1 or a1) and (c2 or a2) ineither order)因为定义中并没有坚持说T1 abort了</p>
</li>
</ol>
<p>可见2对于P1的理解，明显比1要looser，这样理解意味着允许的异常比1要多。所以不严格。</p>
<p>ANSI定义的四种隔离级别就是根据这三种异常定义的：</p>
<table>
<thead>
<tr>
<th>Isolation level</th>
<th>P1</th>
<th>P2</th>
<th>P3</th>
</tr>
</thead>
<tbody><tr>
<td>Read uncommitted</td>
<td>Possible</td>
<td>Possible</td>
<td>Possible</td>
</tr>
<tr>
<td>Read committed</td>
<td>Impossible</td>
<td>Possible</td>
<td>Possible</td>
</tr>
<tr>
<td>Repeatable read</td>
<td>Impossible</td>
<td>Impossible</td>
<td>Possible</td>
</tr>
<tr>
<td>Anomaly serialable</td>
<td>Impossible</td>
<td>Impossible</td>
<td>Impossible</td>
</tr>
</tbody></table>
<h4 id="其他的隔离方法"><a href="#其他的隔离方法" class="headerlink" title="其他的隔离方法"></a>其他的隔离方法</h4><p><strong>cursor isolation</strong>：</p>
<p>主要用在SQL数据库中，为了解决<strong>lost- update-anomaly</strong>：比如说T2即使commit了，它的更新缺丢失了。举例：</p>
<p>P4: r1[x]…w2[x]…w1[x]…c1</p>
<p>这样子，x的最终值是T1修改的值，T2的wirte操作如同丢失一般。</p>
<p>数据库中，游标（cursor）实际上是一种能从包括多条数据记录的结果集中每次提取一条记录的机制。在这里，其实是利用这种方法，结合读写锁操作，T1读到某个item时，执行上锁，直到它commit，再放锁，保证cursor知道的记录在T1 commit之前不会被其他T2读到并修改，也就不会让T2感到自己的更新丢失。</p>
<p>很明显，这个方法并不通用，也不方便。</p>
<p>下面着重讲一下论文提出的snapshot isolation，也就是基于快照的隔离</p>
<h4 id="snapshot-isolation（SI）"><a href="#snapshot-isolation（SI）" class="headerlink" title="snapshot isolation（SI）"></a>snapshot isolation（SI）</h4><p>属于多版本并发控制（multiversion concurrencycontrol）的一种</p>
<ol>
<li><p>写在一个带锁的缓冲区</p>
</li>
<li><p>读来自“snapshot”</p>
</li>
<li><p>只有当<strong>没有**</strong>write-writeconflict**时候，才会提交commit</p>
</li>
</ol>
<p>详细说明如下：</p>
<p>一个transaction T1会在snapshot中读取数据，并获得一个时间戳start-timestamp，这个时间戳可以是该transaction读取数据前的任何时间。在执行过程中commit之前，T1的更新操作对其他transaction 是不可兼得当T1要commit时，会获得一个commit-timestamp，大于已有的所有sts和cts，T1要想成功commit，就要保证在T1.sts—T1.cts之间，没有其他transaction commit并且该transaction修改过T1修改过的数据（write-write conflict），若有，则T1 abort掉。这个特点称作“<strong>First commit wins</strong>”。</p>
<p>当T1成功commit后，它的更新操作会对其他sts大于T1cts的transactions可见。</p>
<p><strong>值得注意的是**</strong>SI<strong><strong>并不是</strong></strong>serialable<strong>**的</strong>。考虑以下情况：</p>
<p>Begin: x=y=50</p>
<p>T1 read x=50, write y = 40;</p>
<p>T2 read y = 50, write x = 40;</p>
<p>Commit T1,T2.</p>
<p>两者并没有write-write conflict，所以可以成功commit，这就引发了异常“short fork”，即中间有一段状态在T1T看来并不是一致的，直到两者都commit后，所有状态才又保持一致。这种异常在serialability中就不会出现。</p>
<p><strong>SI**</strong>的优点**：</p>
<p>SI surely can be benefit to read-only and short-running updatetransition, because it never block read-only transition and readers do notblock updates.</p>
<p>避免了传统保证一致性时使用锁的开销，使得只读transaction不会阻塞reader transaction.</p>
<h2 id="let-6-2pc"><a href="#let-6-2pc" class="headerlink" title="let 6 2pc"></a>let 6 2pc</h2><p>在分布式系统中保证多个tx的原子性和并发能力.<br><strong>2PC</strong>:<strong>流程</strong>：1. Voting. 每个参与者准备cmt,若可以,则上锁,并将投票结果写在Permanent Storage(PS)上,之后返回投票；2. Committing. TC(transaction coordinator)收集所有投票,记录在PS上,并将结果广播给参与者,参与者实际执行cmt或者abort.<br><strong>Time Out</strong>: 在cmt阶段,time out on TC, 则投cmt的参与者需要执行termination协议（发送msg给别的参与者请求信息,可以解决除TC挂掉之外的大部分failure）<br><strong>Crash &amp; Reboot</strong>: TC和参与者都根据log信息来决定自己的操作,如果参与者在log中发现了cmt信息,则需要执行termination协议来决定；failure后恢复时间长（需要发msg）<br><strong>并发控制和2PC</strong>：参与者对读写使用2PL/SI,用2PC进行cmt.<br><strong>Sinfonia:</strong> 共享数据存储服务；横跨多个server；地址空间划分；保证一致性的复制. <strong>Motivation:</strong><br>传统的分布式设计是基于message-passing,需要有复杂的协议来处理；Sinfonia可以使host以一种容错,可扩展,一致的方式共享应用数据,且无需考虑复杂协议.<br>2pc需要多轮网络交互，开销较大<br><strong>Mini-transaction**</strong>背景<strong>: 传统2PC情况下, TC先执行一个transaction,这需要让participants执行一到多次transactions（读取或修改数据项）,在这次transaction结束后,TC执行2PC；在Sinfonia中,TC在application node,而participants在memory node中；如果一次transaction的行为不会影响到TC的决定的话,TC其实可以直接将这些操作放在2PC的第一个phase中执行（piggyback）,从而减少了一次round-trip 沟通所花费的时间；piggyback条件：最后一次action不会影响TC关于abort 或者commit的决定,最后一次action对TC的影响,participant已经知道.</strong>Mini-tx<strong>**语义</strong>：cmp, read, write. 1.用cmp检查数据. 2.若全部匹配,则用read取数据/用write修改. 3.否则abort. <strong>优点</strong>：提供原子性和并发控制;相比传统tx, 灵活性和通用性下降,但是有更少的网络round-trip time; efficiency高. <strong>应用</strong>：原子的交换、原子的读多个数据、验证cache然后修改…<strong>TC</strong>:运行在application结点,而不是mmy结点,且TC不记log,由参与者记；将TC和app绑定,节省了网络RTT；可以容忍app结点挂掉,不会有不一致；mmy 结点挂掉,也不影响可用性和持久性；解决传统2PC下,TC挂掉后block问题. <strong>Recovery coordinator</strong>: 1.询问参与者现存vote; 2. Cmt iff 所有投票为yes. || mmy结点挂掉会block tx, 需要等其恢复.</p>
<h4 id="Intro-1"><a href="#Intro-1" class="headerlink" title="Intro"></a>Intro</h4><p><strong>Sinfonia: A NewParadigm for Building Scalable Distributed Systems</strong></p>
<p>作者称Sinfonia是一个新的用于建设分布式系统的paradigm(范式)，可以理解是分布式系统底层的架构设计，或者是底层提供的一种服务。</p>
<p>传统的分布式设计是基于<strong>message-passing</strong>，在这种设计中各个进程间通过传递消息来共享数据。这种方式的缺点是需要借助复杂的协议来处理分布式状态，协议包括replication, ﬁle data and metadata management, cache consistency, andgroup membership.这些都不简单。</p>
<p>Sinfonia 这种服务能够让host可以以一种容错，可扩展，一致的方式共享应用数据。开发者使用这种服务就不必考虑复杂的message-passing 协议。</p>
<h4 id="Sinfonia-架构"><a href="#Sinfonia-架构" class="headerlink" title="Sinfonia 架构"></a>Sinfonia 架构</h4><p><img src="file:///C:%5CUsers%5Cuser%5CAppData%5CLocal%5CTemp%5Cmsohtmlclip1%5C01%5Cclip_image002.jpg" alt="img"></p>
<p><strong>Sinfonia</strong> <strong>设计思想</strong>：</p>
<ol>
<li><p>Distributed Shared Memory as a Service</p>
</li>
<li><p>包含多个memory node ，展示出flat, fine-grained address spaces</p>
</li>
<li><p>运行在application node上的进程，通过user library 来访问服务</p>
</li>
</ol>
<p>其核心技术是mini-transactions。</p>
<h4 id="mini-transaction"><a href="#mini-transaction" class="headerlink" title="mini-transaction"></a>mini-transaction</h4><p>一句话描述</p>
<p>A lightweight, short-lived type of transaction</p>
<p><strong>改良</strong></p>
<p>传统2PC情况下，一个TC先执行一个transaction，这需要让participants执行一到多次transactions（读取或修改数据项），在这次transaction结束后，TC执行2PC。</p>
<p>在Sinfonia中，TC在application node，而participants在memory node中。作者观察到，如果一次transaction的行为不会影响到TC的决定的话，TC其实可以直接将这些操作放在2PC的第一个phase中执行（piggyback），从而减少了一次round-trip 沟通所花费的时间。</p>
<p><strong>可以**</strong>piggyback<strong>**的情况</strong></p>
<ol>
<li><p>最后一次action不会影响TC关于abort 或者commit的决定</p>
</li>
<li><p>最后一次action对TC的影响，participant已经知道</p>
</li>
</ol>
<p>​                </p>
<p><strong>细节</strong> mini transaction 包含</p>
<ol>
<li><p>set of compare items</p>
</li>
<li><p>set of write items</p>
</li>
<li><p>set of read items</p>
</li>
</ol>
<p>semantics:</p>
<ol>
<li><p>检查compare items中data（equality）</p>
</li>
<li><p>如果全部匹配</p>
</li>
</ol>
<p>​               1）获取read items中的数据</p>
<p>​               2）写write items中的数据</p>
<ol start="3">
<li>否则Abort</li>
</ol>
<p><strong>意义：</strong>相比传统的transaction，less flexible and general purpose，但是拥有fewer network round-triptime。</p>
<p>典型的应用：</p>
<ol>
<li><p>Validatecache using compare items and write if valid</p>
</li>
<li><p>Use compare items to validate data withoutread/write items; commit indicates validation was successful for read-onlyoperations</p>
</li>
</ol>
<p>总的来说，非常抽象。基本上是提供了一套机制，开发者可以利用这套机制来定制自己的策略，由于限制比较大，所以minitransaction的通用性不强，相比传统的transaction，它的expressiveness也不够好。但是在数据中心底层一些应用中还是有其适用性的（作者给予Sinfonia也实现了两个应用）。并且有比较好的efficiency。</p>
<p>基本可以看作是</p>
<p>Trade off expressiveness for efficiency</p>
<h4 id="Sinfonia的2PC和recovery-问题"><a href="#Sinfonia的2PC和recovery-问题" class="headerlink" title="Sinfonia的2PC和recovery 问题"></a>Sinfonia的2PC和recovery 问题</h4><p>如上所述哦，Sinfonia中TC运行在application node上，而不是在memory node上。</p>
<p><strong>Fault Tolerance</strong> ：</p>
<p>app node 挂掉，不会有数据丢失的情况（即，inconsistency）。</p>
<p>memory node crash，也不应该影响availability和durability。</p>
<p>这些保护都相应的由disk image，logging，replication，backup来实现。</p>
<p><strong>传统**</strong>2PC**</p>
<p>TC需要记录log，来追踪transaction 是否commit，TC crash 后，会阻塞所有的transaction。</p>
<p>这在分布式场景下明显不可取，并且Sinfornia 的TC是在App node上，app node相比memory node 要less reliable。</p>
<p>Sinfornia使用单独<strong>Recoverycoordinator**</strong>（<strong><strong>RC</strong></strong>）**</p>
<p>TC不再记录log，而是由所有participants记log，一个transaction只有在所有participants 的log都记录yes时才会commit。</p>
<p>disaster发生后，RC做clean-up工作：</p>
<p> 1）询问所有participants的existing vote</p>
<p> 2）commit iff allvote yes</p>
<p>若是memory node挂掉，则需要transaction blocks，用redo log恢复自己。</p>
<h2 id="let-7-paxos"><a href="#let-7-paxos" class="headerlink" title="let 7 paxos"></a>let 7 paxos</h2><p>拜占庭将军问题</p>
<h4 id="Intro-2"><a href="#Intro-2" class="headerlink" title="Intro"></a>Intro</h4><p>分布式系统有消息传递和内存共享两种模型。而在基于消息传递的分布式系统中，可能会发生进程被挂起，消息丢失，重发等情况。所以需要一致性算法保证一致性，使得分布式系统中即使发生了上述问题，也可以就某一个值的最终状态达到一致。当然算法比较复杂，（也就是Sinfornia论文中踩消息传递的理由）这篇论文作者用大白话给人复述了一遍。</p>
<h4 id="算法目标和前提"><a href="#算法目标和前提" class="headerlink" title="算法目标和前提"></a>算法目标和前提</h4><p>一致性算法保证所有提出的决议中，有一个决议会被最终选择。</p>
<p>如果一个决议被选中，那么所有processes最终都会知道这个被选中的决议。</p>
<p>最终保证正确性和容错性，但是不保证算法会终结。</p>
<p><strong>前提</strong>假设代理之间用消息通信，采用异步，非拜占庭模型。非拜占庭模型指的是允许消息的丢失或者重复，但是不会出现内容损坏的情况。</p>
<h4 id="算法细节"><a href="#算法细节" class="headerlink" title="算法细节"></a>算法细节</h4><p>一致性算法中的四个角色：</p>
<ol>
<li><p>client：make a request</p>
</li>
<li><p>proposer: 提出者 Get a requestand run the protocol</p>
</li>
<li><p>Leader = elected Coordinator</p>
</li>
<li><p>acceptor：批准者 Remember thestate of the protocol</p>
</li>
<li><p>Quorum = any majority of Acceptor</p>
</li>
<li><p>learner：接受者 When agreementhas been reached, a Learner executes the request and/or sends a response backto the Client</p>
</li>
</ol>
<h4 id="详细过程（具体可以参考PPT）"><a href="#详细过程（具体可以参考PPT）" class="headerlink" title="详细过程（具体可以参考PPT）"></a>详细过程（具体可以参考PPT）</h4><p>主要分成申请阶段和沟通阶段。</p>
<ol>
<li><p>client 发请求给proposer，proposer选出Leader，Leader利用编号N（比之前的都要大）发送提议（proposal）给多数 acceptor（quotum）。</p>
</li>
<li><p>对acceptor来说，若收到的编号ID &gt; 之前所有的，则1）返回之前收到的最大的proposal以及一个value，2）promise 忽略所有IDs &lt;N 的proposal；否则，直接忽视（proposal被拒绝）</p>
</li>
<li><p>对leader来说，一旦受到足够的promise，1）为proposal设置一个value V（可以使决定好的，也可以是新的）2）发送accept request 给quorum，并附带选中的V。这就到了<strong>沟通阶段</strong></p>
</li>
<li><p>acceptor ，如果promise保留着，就1）注册value V ，2）发送accepted message给proposer/learner,否则就忽略accept request这条message</p>
</li>
<li><p>最后，learner收到信息，给client 做出反应，并对request 做出action</p>
</li>
</ol>
<h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><p>To make a change to the system</p>
<ol>
<li>Tell the proposer(leader) the event</li>
</ol>
<p>(NOTE: these requests may occur concurrently)</p>
<ol>
<li><p>The leader picks its next highest ID and asksproposal to all the acceptors with that ID</p>
</li>
<li><p>When the majority of acceptors accept the proposal,accepted event are sent to learners</p>
</li>
<li><p>The learners do event (e.g., update system state)</p>
</li>
</ol>
<p>分布式系统的容错.<br><strong>ReplicatedState Machine(RSM)</strong>: may无法保证顺序一致性à采用基于primary/backup,可以保证SC,但是failure时,爆炸,因此需要一个分布式容错的共识协议.即一般情况用一套,如2PC,有failure之后用一套,如Paxos<br>.<strong>共识算法要求</strong>：正确性（所有节点同意同一个值,且该值是由某个节点提出的）、容错性（允许少数节点fail）、可终止；分布式异步场景下,无法同时满足.<br><strong>Paxos</strong>: 分布式系统有消息传递和内存共享两种模型,Paxos是基于msg传递的容错共识协议.<br><strong>前提</strong>：代理之间用消息通信,采用异步,非拜占庭模型(msg有丢失、重复,无损坏).<br><strong>目标</strong>：保证共识的正确性和容错性,无法保证算法终结.<br><strong>成员</strong>：client提出请求；proposer接收请求,发起协议；acceptor记录协议状态,共识协议投票；learner执行共识结果.<br><strong>流程：<br>**</strong>1.<strong>client 发请求给proposer,proposer选出Leader,Leader利用编号N（比之前用的都要大）发送提议proposal给多数 acceptor即quorum；提议要被quorum接收后才能真正成为leader. **2.</strong> 对acceptor来说,若收到的编号N&gt;之前见过的所有的,则1）返回之前收到的最大的proposal以及一个value,2）promise 忽略所有IDs &lt;N 的proposal；否则,直接忽视（proposal被拒绝）<br><strong>3.</strong> 对leader来说,一旦受到足够的promise,1）为proposal设置一个value V（可以是决定好的,也可以是新的）2）发送accept request 给quorum,并附带选中的V.<br><strong>4.</strong>Acceptor, 如果promise保留着(N仍然是自己见过的最大值),就1）注册value V,2）发送accepted message给proposer/learner, 否则就忽略accept request这条msg.<br><strong>5.</strong>learner收到信息,给client 做出反应,并对request 做出action.<br><strong>补充</strong>：期间协议失败,则delay and restart；一个server可能有多个角色；acceptor要记录Nh,Na,Va到log中,以便recover.</p>
<h2 id="let-8-Distributed-File-Systems"><a href="#let-8-Distributed-File-Systems" class="headerlink" title="let 8 Distributed File Systems"></a>let 8 Distributed File Systems</h2><p><strong>1.NFS</strong>：最传统的网络文件系统，支持一些简单的操作，如read和write。<a href=""><strong>设计目标如下</strong>：</a>a)任何机器都能成为客户端和服务器；支持没有硬盘的工作站（历史原因）；支持不同配置的机器；访问透明（用户访问远程的资源就像访问本地的一样）；可以从错误中恢复；具有较高的性能。<strong>Mounting protocol</strong>:所有请求访问暴露出来的目录树，具体地，客户端发送文件的路径名到服务器，服务器返回handle。<strong>关于validation</strong>：出现不一致的问题，解决方法是时间戳，当文件被打开或者服务器被连接，则比较时间戳，发现远端的时间戳更新，则invalidate掉缓存中的数据。同时，对于打开的文件，固定每3秒invalidate一次，文件夹则是30秒invalidate一次。<strong>提升读性能：</strong>每次都多返回一些读数据，预读，避免程序反复发送读邻近数据的请求。<strong>存在的问题：</strong>主要是一致性问题和服务器无状态带来的一些问题。<strong>2.GFS：</strong>运行于廉价的普通硬件上的可扩展的分布式文件系统，用于大型的、分布式的、对大量数据进行访问的应用。<strong>架构：</strong>一个GFS包括一个主服务器和多个块服务器，这样一个GFS能够同时为多个客户端应用程序提供文件服务。文件被划分为固定的块，由主服务器安排存放到块服务器的本地硬盘上。<strong>设计目标如下</strong>：可拓展性、适用于数据密集型、提供fault-tolerant<strong>设计假设：</strong>传统文件系统不再适用，因为通常支持的文件都比较小；failures非常常见；存储的文件都非常大；大多数写操作都是append而不是overwrite；大多数workload都是只读的。<strong>块和块服务器：</strong>块大小一般是64MB，块的handle由主服务器负责分发，块的本质是一个Linux系统中的本地文件，每一个块文件在不同的Node中进行冗余。<strong>主服务器：</strong>主服务器会记录存放位置等数据，并负责维护和管理文件系统，包括块的租用、垃圾块的回收以及块在不同块服务器之间的迁移。但是，主服务器不会持久化所有块的位置，因为块的位置在各个块服务器可能会发生变动，持久化后容易发生不一致的问题，解决访问只能是定期访问所有的块服务器。同样需要对主服务器进行备份冗余，通过心跳信息定期与所有节点进行通信。<strong>块规模：</strong>较大的原因1.减少客户端和主服务器之间的交互。因为读写同一个块只要在开始时向主服务器请求块位置信息，大的块可以减少请求的次数，对于读写大型文件这种减少尤为重要。2.客户端在一个给定的块上很可能执行多个操作，和一个块服务器保持较长时间的TCP连接可以减少网络负载。3.减少了master上保存的元数据的规模，从而使得可以将metadata放在内存中。<strong>读写操作：</strong>读操作:与主服务器建立连接;获取元数据（主要是handle）；获取handle所在的位置；与相应的块服务器建立连接。写操作：第一阶段（数据流阶段），发送数据，发送到多个块服务器，数据暂时存在缓存中；第二阶段（控制流阶段），写入数据，发送完成后，各个块服务器将数据写入硬盘中，这个过程中会有一个primary<a href="">块服务器</a>来接受客户端的写操作请求，并通知各个其他块服务器序列化写操作。<strong>3.Chubby：</strong>Chubby能够提供机制使得client可以在Chubby service上创建文件和执行一些文件的基本操作。说它是分布式的文件系统，是因为一个Chubby cell是一个分布式的系统，一般包含了5台机器，整个文件系统是部署在这5台机器上的。从更高一点的语义层面上，Chubby是一个针对松耦合的分布式系统的lock service。它提供了文件访问、事件通知和文件锁等服务。<strong>适用于：</strong>粗粒度的、长期持有的锁；数量较少的数据，如系统的配置文件。<strong>主服务器：</strong>用Paxos协议来选择主服务器；如果超过了租约事件或者发生了failures,则重新用Paxos来选择主服务器。<strong>文件系统的接口：</strong>Chubby的文件系统和UNIX类似，例如在文件名“/ls/foo/wombat/pouch”中，ls代表lock service，这是所有Chubby文件系统的共有前缀；foo是某个单元的名称；/wombat/pouch则是foo这个单元上的文件目录或者文件名。<strong>锁服务：</strong>Chubby系统本质上就是一个分布式的、存储大量小文件的文件系统，它所有的操作都是在文件的基础上完成。Chubby最常用的锁服务中，每一个文件就代表一个锁，用户通过打开、关闭和读取文件，获取共享（Shared）锁或独占（Exclusive）锁；选举主服务器过程中，符合条件的服务器都同时申请打开某个文件并请求锁住该文件；成功获得锁的服务器自动成为主服务器并将其地址写入这个文件夹，以便其他服务器和用户可以获知主服务器的地址信息。这里的锁是建议锁不是强制性的锁，有更大的灵活性。<strong>缓存机制：</strong>客户端会保存文件数据和节点的元数据在一个一致的，采用的write-through的缓存中。在客户端保存一个和单元上数据一致的本地缓存。当某个文件数据或者元数据需要修改时，主服务器首先将这个修改阻塞；然后通过查询主服务器自身维护的一个缓存表，向对修改的数据进行了缓存的所有客户端发送一个Invalidation。客户端收到这个无效标志后会返回一个确认Acknowledge，主服务器在收到所有的确认后才解除阻塞并完成这次修改。</p>
<h2 id="let-9-data-parallel"><a href="#let-9-data-parallel" class="headerlink" title="let 9 data parallel"></a>let 9 data parallel</h2><p><strong>1.MapReduce</strong> 核心编程结构map函数：Map(input shard)-&gt;intermediate(k/v pairs)，将数据划分为M个shards，然后将相同Key的value集合起来发送到reduce阶段。Reduce函数：intermediate(k/v<br>pairs)-&gt;(results)利用划分函数将中间结果划分到R个块上，如hash(key) mod R，然后将排序后的键值对根据特定Key合并value值。<strong>一次MapReduce的流程：</strong>1.把输入文件划分为M份（M为用户定义），每一份通常有16MB到64MB；然后使用fork将用户进程拷贝到集群内其它机器上。2.user program的副本中有一个称为master，其余称为worker，master是负责调度的，为空闲worker分配任务，worker的数量也是可以由用户指定的。3.被分配了Map作业的worker，开始读取对应分片的输入数据，Map作业数量是由M决定的，和split一一对应；Map作业从输入数据中解析出键值对，每一个键值对都作为参数传递给map函数，map函数产生的中间键值对被缓存在内存中。4.缓存在内存中的中间键值对会被定期写入本地磁盘，而且被分为R个区，R的大小是由用户定义的，将来每个区会对应一个Reduce作业；这些中间键值对的位置会被通报给master，master负责将信息转发给Reduce worker。5.master通知分配了Reduce作业的worker它负责的分区在什么位置，当Reduce worker把所有它负责的中间键值对都读过来后，先对它们进行排序，使得相同键的键值对聚集在一起。因为不同的键可能会映射到同一个分区也就是同一个Reduce作业，所以排序是必须的。6.reduce worker遍历排序后的中间键值对，对于每个唯一的键，都将键与关联的值传递给reduce函数，reduce函数产生的输出会添加到这个分区的输出文件中。7.当所有的Map和Reduce任务都完成了，master唤醒user program，MapReduce函数调用返回user program的代码。<strong>Locality:</strong>输入输出文件都建立在GFS上；在GFS的chunkservers上运行协调计算和存储，提高数据的本地性。<strong>Fault Tolerance:</strong>Master会定期ping所有的worker，如果在特定的时间内收不到回复则认为该workerfailed，然后reset该worker并re-execution；如果Mater出现failure则由GFS根据checkpoint进行恢复。<strong>Straggler:</strong>执行速度慢的worker会极大地影响整个工作的时长，出现这种情况的原因喝多：如资源被其他任务抢占、硬盘坏道、没有打开缓存等等；解决方法：copy该执行缓慢worker的状态和数据，由其他worker执行，谁先执行完成就用谁的结果。<strong>存在的问题：</strong>只能完成一个完整的mapreduce才能进行下一个阶段，这一个阶段可能时间很长，对时效性高的需求不适用，解决办法是提出了caffeine的框架。</p>
<p><strong>2.Dryad</strong>：将计算过程表现为图，其中顶点代表计算，边代表通信通道，每个顶点有若干条入边和出边。因为很多程序用数据流图的形式表示更合适，所以该模型基础就是将程序执行过程表示为一个有向无环图。用户通过实现自定义的Vertex节点来执行定制的运算逻辑，而节点之间通过各种形式的数据通道传输数据，用户的运算逻辑本身通常是顺序执行的，而与分布式相关的逻辑则由Dryad框架来实现。<strong>运行过程：</strong>1.顶点：可以运行任意的程序代码，通过tcp各个计算节点之间通过例如文件，管道，网络等形式的通道交换数据，并定期与JM通信；2.JM在应用程序内部维护了一个基于DAG图模型的计算节点依赖关系图，JM通过命名服务器NS来获取可用的服务器列表，而后通过在这些服务器上运行的守护进程Daemon来调度和执行计算节点Vertex。3.守护进程Daemon，运行各个顶点上的计算任务。<strong>JM的调度：</strong>如果输入数据已经准备好，一个顶点可以随时进行计算，同时尽量将数据置于计算节点的本地，提高数据的locality。<strong>Fault-tolerance：</strong>如果节点出现fail，则重新运行一次；如果节点的输入数据消失，则递归地执行之前依赖的节点计算过程，直到所有输入数据重新准备好；如果某个节点计算很慢，和mapreduce中一样，起其他一个节点执行相同的代码和输入数据，谁先完成则使用谁的结果。<strong>优势：</strong>适合更加大型复杂的任务，有向图的形式对于多个阶段的任务可以减少不必要的通信和数据的冗余。更加灵活，可以支持join操作。</p>
<h2 id="let-10-Pregel-amp-GraphLab"><a href="#let-10-Pregel-amp-GraphLab" class="headerlink" title="let 10 Pregel&amp;GraphLab"></a>let 10 Pregel&amp;GraphLab</h2><p>图并行计算算法具有dependency graph，predictableupdates，iterative computation等特征，mapreduce等并行计算模型在实现这些算法是非常的困难和低效(mapreduce处理迭代很低效率)，所以本文在现有的图计算模型graphlab和pregel的基础上提出了distributed graphlab。</p>
<p><strong>Pregel</strong>采用了bulk synchronous parallelmodel, vertex之间的通信通过message-passing实现。对于每个vertex来说计算过程如下：1，接受上一步收到的邻居节点发送的消息；2，执行用户定义的update function；3，修改自身的value；4，如果该节点是active的（尚未收敛），向邻居节点发送消息；5，如果该节点是inactive的，投票halt。所有节点之间的barrier synchronization由master来完成，当所有vertex投票halt的时候，计算终止。<strong>架构</strong>:master-slave.<strong>缺点</strong>:straggler;通信量大,有不必要的通信.<strong>Pregel的整体性能受计算最慢的机器限制</strong>。同步计算，每次都要给邻居发消息(即使不需要)，造成不必要的通信，并且每次迭代都会受最慢的机器限制。</p>
<p><strong>GraphLab</strong>:async计算(邻居改变时才evaluate comdition,同步需要每阶段对每个顶点evaluate);通过shared mmy通信; 每个vertex可以直接访问自身及邻居边、邻居节点的数据,可以根据自身的执行结果来调度邻居vertex的后续执行; serializability通过防止邻居节点同时执行来实现. <strong>Data Graph</strong>:存在mmy,支持快速随机查找;Ghost顶点维持顶点局部结构,复制远端数据.<strong>Partition</strong>: 首先使用random hashing等方法将graph分为k部分（k远大于机器数）,每部分叫做一个atom,作为一个文件存储在分布式存储系统上（HDFS等）.每个atom中还需要存储ghosts信息,k个atom之间的连接关系存储在一个叫做meta-graph的文件中.在将graph信息load进机器内存时,先按照机器数量对meta-graph做一个balanced partition,然后每台机器按照对应atom的信息建立graph的局部结构. <strong>更新函数</strong>:用户定义,运行在顶点,只涉及顶点scope数据. <strong>Scheduler</strong>:决定顶点更新顺序(FIFO/Priority/Round Robin). <strong>一致模型**</strong>/<strong>**并行度</strong>:1. Full consistency:点、边、邻居节点.2. Edge consistency:点、边.3. Vertex consistency:点. <strong>执行</strong>engine**: 负责执行update function和sync operation,维持一个被调度的vertex集合,并按照给定的一致性模型来保证serializability. 1. chromatic engine:按照对应的serializability要求对节点使用贪心法进行染色,相同颜色的节点可以同步执行,同一颜色的节点执行完成之后才可以开始执行下一种颜色的节点. 2. distributed locking engine：每个vertex有一个read-write lock,在vertex consistency中只需要获得central vertex的write-lock；在edge consistency中需要获得central vertex的write-lock及相邻两个vertex的read-lock；在full consistency中需要获得central vertex和相邻两个vertex的write-lock; 3.dead lock可以通过按一定顺序拿锁来获取. 4. pipelined locking and prefetching: 为了减少远程锁请求和数据同步的时延, 每台机器维持一个pipeline, pipeline中的所有vertex同时请求锁,哪个vertex先拿到锁就先执行,而不是所有vertex按顺序执行.</p>
<p><strong>Graphlab</strong>是一种异步并行模型，vertex之间的通信通过shared-memory实现。每个vertex可以直接访问自身及邻居边、邻居节点的数据，可以根据自身的执行结果来调度邻居vertex的后续执行，serializability通过防止邻居节点同时执行来实现。</p>
<p><strong>Distributed graphlab</strong>基于graphlab实现，改模型主要包含三个部分：data graph, update function, synchronization. 其中data graph的划分和构造分两步实现：首先使用random hashing等方法将graph分为k部分（k远大于机器数），每部分叫做一个atom，作为一个文件存储在分布式存储系统上（HDFS等）。每个atom中还需要存储ghosts信息，k个atom之间的连接关系存储在一个叫做meta-graph的文件中。在将graph信息load进机器内存时，先按照机器数量对meta-graph做一个balanced partition，然后每台机器按照对应atom的信息建立graph的局部结构。Updatefunction是一个用户自定义的程序，它根据vertex自身及邻居数据来迭代更细当前vertex的数据。</p>
<p>Distributed graphlab中分三种层次的serializability：1,Full consistency,  2, edgeconsistency,  3,vertex consistency</p>
<p><strong>distributed graphlab**</strong>中有两种执行engine，engine**负责执行update function和sync operation，维持一个被调度的vertex集合，并按照给定的一致性模型来保证serializability.</p>
<p>（1））chromatic engine:按照对应的serializability要求对节点使用贪心法进行染色，相同颜色的节点可以同步执行，同一颜色的节点执行完成之后才可以开始执行下一种颜色的节点。对于vertex consistency，所有节点染成同一颜色；对于edge consistency，任何两个相邻节点不能染成相同颜色；对于full consistency，距离为2及2以内的节点不能染成相同颜色。</p>
<p>（2））distributed locking engine：每个vertex有一个readers-writer lock，在vertex consistency中只需要获得central vertex的write-lock；在edge consistency中需要获得central vettex的write-lock及相邻两个vertex的read-lock；在fullconsistency中需要获得central vertex和相邻两个vertex的write-lock；deadlock可以通过按一定顺序拿锁来获取，具体来说，拿锁顺序有machine ID及vertex ID（owner（v），v）决定。</p>
<p>为了减少remote lock acquisition和data synchronization的时延，采用了pipelined lockingand prefetching: 每台机器维持一个pipeline，用于存放该机器上所有请求锁但是还没有获得锁的vertex，当一个vertex获得锁并完成数据同步后就离开这个pipeline然后有一个工作线程去执行。（即pipeline中的所有vertex同时请求锁，哪个vertex先拿到锁就先执行，而不是所有vertex按顺序执行，具体实现中采用的不是readers-writer lock，而是通过操作callback来实现的readers-writer lock的一个非阻塞版本）</p>
<h2 id="lec-11-PowerLyra"><a href="#lec-11-PowerLyra" class="headerlink" title="lec 11 PowerLyra"></a>lec 11 PowerLyra</h2><p>不同的划分对网络IO会有较大影响。</p>
<p>现实中的图大都是skewed，即大多数节点的邻居较少，少部分节点的邻居非常多。在一个机器集群中进行图的分布式计算时必然涉及到对图的分割。现有的分割图的方案主要有两种，即edge-cut和vertex-cut。Edge-cut将vertex均匀地分布在各个机器上，每个vertex对应的edge也随vertex分布在相应的机器上，如果同一个edge的对应的两个vertex分布在两台机器上，则该edge在这两台机器上各有一个拷贝，在edge-cut中，大部分vertex所需要的计算资源都在本地机器，所以在计算时可以充分利用locality，但是这种分割方案缺乏parallelism，度数特别大的可能成为瓶颈。Vertex-cut将edge均匀地分布到各个机器上，如果一个vertex的几个edge在不同的机器上，那么该vertex在这几台机器上都会有拷贝，在vertex-cut中，一个vertex可能分布在好几台机器上，所以计算时可以充分利用parallelism，将一个vertex的负载分散到多个机器上，但是缺乏locality，网络时延大。现有的图分割方案大都是“one size fit all”，所有的节点被平等对待，为了使尽可能地利用locality和parallelism，本文将low-degree和high-degree的区别对待，将edge-cut和vertex-cut结合起来，提出了hybrid-cut。在hybrid-cut中，对于low-degree vertex，尽量减少vertex的mirror数量，尽可能地利用locality；对于high-degree vertex，主要考虑负载均衡，是计算并行进行。具体来说，先按vertex编号及机器数量将vertex均匀地哈希到所有机器上，所有edge按照其入度跟随vertex分布到相应机器上，在这个过程中，记录所有vertex的入度；然后设定一个threshold，入度超过这个threshold的vertex是high-degree vertex；对于high-degree vertex，将其入边按照source vertex重新hash到对应机器，及high-degree vertex的入边跟随其source vertex；最后建立mirrors来完成每台机器上local graph的创建。</p>
<p><strong>通信</strong>:1.高度数,&lt;= 4*mirrors(有一个mirror就有4次 Gather2次,Scatter2次)(powerGraph是5,将apply和scatter1结合);2.低度数, &lt;=1*#mirrors (Scatter, master-&gt;mirror);若G&amp;S阶段需要双向edge,根据需要逐步在G&amp;S阶段增加master和mirror之间的通信,总数最多不超过4次.</p>
<p>按照hybrid-cut对图进行划分后，计算时所有vertex都采用GAS计算模型，在high-degree vertex中，负载被分散在多台机器上，从而保证负载均衡；在low-degree vertex中，所有计算都在本地机器完成，只在scatter阶段需要master和mirror之间的通信。</p>
<p>对于low-degree vertex，上述模型只考虑了单方向的locality，为了使改模型更通用，可以根据具体算法的需要，在scatter和gather阶段master和mirror之间的通信，但是增加的额外通信message数不会超过3.，</p>
<p><strong>参考ADS 10 -12</strong></p>
<h2 id="let-12-GraphChi"><a href="#let-12-GraphChi" class="headerlink" title="let 12 GraphChi"></a>let 12 GraphChi</h2><p><strong>背景</strong>:分布式程序难写;有效扩展的需求(传统多台机子一次一个task,一个机子一个task的话更易扩展); Cost; Energy. <strong>目标</strong>:单台机子,reasonable时间,大规模图计算.<strong>挑战</strong>:磁盘的随机访问.若以对称邻接文件的方式存储graph, 需要每秒数百万次的磁盘随机访问, SSD也满足不了,因此提出PSW. <strong>Parallel Sliding Windows(PSW)</strong>:1个应用分多个迭代执行完,之前是一个迭代完成整张图的一次计算,现将图分为sub-graph,一个迭代分为多个小迭代执行一个sub-graph,每个小迭代为loadàcomputeàwrite. 流程:1. 所有的顶点依次被编号为1-N, 按顺序分为P个interval,一个sub-graph对应一个interval,每个interval在disk上有一个对应的shard.(每个shard的size足够小,刚好放入内存中; 一个shard中存储其所对应的interval中的所有顶点及其入边; 入边按source顶点排序,使得顶点的出边在每个shard中顶点编号顺序存储). 每个interval需要P次large读, 每次迭代总共需要P2次顺序读. 2. 计算阶段,  本次计算结果下个interval可见,即sub-graph之间是异步的, sub-graph内部均可(不同interval之间同步，同一个interver之内异步). 3.更新写回disk,保证下个shard可见,一次迭代有P2次顺序写.<br><strong>Evolving Graph</strong>:新加入/删除边. 每个interval有一个edge-buffer,新加的edge会被临时存放在edge-buffer中,删除的边被标记为”removed”, 当edge-buffer满了的时候,shard会被在磁盘上recreated,太大的shard会被重新分割为较小的shard,在recreate的过程中,“removed”边会被永久删除. <strong>总结</strong>:PSW可以处理大规模图with很少随机disk访问; 合适的数据结构来scale up</p>
<h2 id="let-13-Tiled-MapReduce"><a href="#let-13-Tiled-MapReduce" class="headerlink" title="let 13 Tiled-MapReduce"></a>let 13 Tiled-MapReduce</h2><p><strong>背景</strong>：多核处理器普及,可以在单机上实现数据并行应用.<strong>并行优化points</strong>:运行算法、可扩展的数据结构、OS交互.<strong>Phoenix</strong>:<br>利用thread实现并行,利用共享地址空间进行通信.<strong>数据结构</strong>：Intermediate Buffer,矩阵结构.行对应thread,列为key；mapper按行,reducer按列,因此同阶段内数据不冲突,无需加锁；表内为指针指向mmy中数据,降低数据传输开销.<strong>缺陷</strong>：1.内存开销大.需要把整个输入数据都放在内存.2.数据locality差.一次处理全部输入数据,cache不停重刷,reduce相当于面对空cache.3.严格依赖barriers. map全部结束才可以reduce.<strong>TMR</strong>：<strong>Tiling策略</strong>：将大的job拆分为小的sub-job,迭代处理,每次一个sub-job,要求reduce函数是Commutative和Associative. <strong>流程</strong>：1.map.只加载部分输入数据,够sub-job用,将结果存入Intermediate Buffer(IB).2. combine.处理IB中的列,存入Iteration<br>Buffer.1&amp;2循环处理所有子任务. 3.reduce.处理Iteration Buffer,存入Final Buffer.<strong>优点</strong>：并发提高；cache、mmy利用率提高.<strong>缺点</strong>：计算次数增大.<strong>优化</strong>：1.Mmy<br>Reuse. mmy中只存当前子任务需要的数据；不同子任务重用input buffer和IB；重用IB之前,将必需数据复制集中到一起,new buffer（不再用指针）；加入compress阶段自动合并不同combine结果,减少内存占用. 2.数据locality. 每个子任务的working set刚好放在最后一级cache里,使map相当于为reduce预取数据；1中在combine阶段集中必要数据；NUCA/NUMA-Aware Scheduler（以往的问题是用多个核来处理同一个job,现在只要让subjob跑在同一个核上,并只用该核的内存即可；先按core分为不同repeater,然后core内再分worker,Repeater的Job Buffer、Intermediate Buffer、Iteration<br>Buffer都是私有的；将subjob切分得小,通过work-stealing 机制,实现repeater的负载均衡）3. Software Pipeline: map阶段能者多劳,负载较为均衡,但是reduce阶段,key数量少时,可能不均衡；下一个子任务的map与前一个的combine阶段无数据依赖,可以pipeline；子任务间重用IB的依赖,可以用dual buffer消除. 4. Thread Pool: 子任务重用线程</p>
<h2 id="let-14-Replication-based-fault-tolerance-Imitator"><a href="#let-14-Replication-based-fault-tolerance-Imitator" class="headerlink" title="let 14  Replication-based fault tolerance(Imitator)"></a>let 14  Replication-based fault tolerance(Imitator)</h2><p><strong>背景</strong>：图变大,由于scale大,运行时间长,容错很重要.<strong>BSP流程(图计算)</strong>：Load计算(master) SendMsg(master和replica)(EnterBarrier, Commit, LeaveBarrier),完成CMT,保证结果在下次迭代可见.<strong>容错技术</strong>：1. MapReduce：Simple re-execution. 分给别的worker执行,谁先commit用谁的.问题：只支持确定性、独立的任务,无法处理强依赖. 2. Spark：使用Resilient Distributed Datasets,粗粒度操作,log只记录操作.问题：没有data,不支持细粒度. 3. Checkpoint. (incremental CKPT) 同步中所有节点在global barrier处生成snapshot,异步中以固定时间间隔启动生成.恢复时从metadata snapshot加载图拓扑,从data snapshot更新点和边状态.问题：大开销,慢恢复（一台机器挂,所有机器都要回滚,带来大量读写、IO、同步开销,需要standby node；受限于单机IO；同步中的global barrier不平衡可通过降低打log频率缓解,但会导致恢复时间变长）<strong>Imitator</strong>:<strong>设计思想</strong>：几乎所有节点都有replicas,并且均匀分布在多台机器上.可以重用它们来做recovery.实现：扩展同步msg部分,添加检测.1.Failure before barrier：enter时发现,新建一个机器取代它,其它机器需要回滚到迭代开始,因为一些消息可能因为crash丢了. 2.Failure after barrier：leave时发现,新建一个机器取代它,redo先前流程.<strong>Fault tolerant (FT) replica</strong>：对没有replica的顶点在load图时建立FT replica；位置保证负载均衡；<strong>selfish<br>vertice</strong>：即无出边,只需建立FT replica,无需和master同步,恢复时从FT拿静态信息,动态信息通过邻居节点重新计算.<strong>Full-state<br>Replica (Mirror)</strong>：于master等价；由于普通的replica缺乏meta信息,选择一个mirror存放,负责恢复maste；贪婪分布在mirror少的node上.<strong>恢复目标：</strong>并行恢复；恢复后状态一致.<strong>恢复策略</strong>：<strong>1.<br>Rebirth</strong>.<strong>规则</strong>:master恢复replicas; mirror恢复master;需要standby结点. <strong>流程</strong>：Reloading(活着的点并行检查哪些结点挂掉了,并行发送消息给standby帮助其恢复)-Reconstruction(计算,重构图)-Replay(重做操作来得到最新状态,比如active,和mirror同步时可能还未收到该消息,于是在收到后要发给mirror,这样恢复时mirror会发给它replay) <strong>缺点</strong>：需要Standby；恢复慢,reconstruct/replay阶段不可并行. <strong>2. Migration</strong>.<br>挂后将它所属的数据分布到剩余机器上,比如master挂对应mirror就被提升为master,在它们之上重建图结构.<strong>流程</strong>：Reloading(提升mirror,广播New replicas/Edges from old replicas to the new master/Edges between<br>masters)-Reconstruction(利用消息在剩余机器上重构)-Replay(只需激活被提升成master的节点,自己有激活消息,直接恢复) <strong>优点</strong>：三个阶段都可并行.<strong>总结</strong>：低开销（利用现有的replica）、恢复快（并行恢复）</p>
<h2 id="lec-extra-sandbox"><a href="#lec-extra-sandbox" class="headerlink" title="lec extra sandbox"></a>lec extra sandbox</h2><p>Royan提供了一个分布式的sandox，利用intel SGX技术保护各个sandbox不被恶意的程序估计；而各个sandbox(nacl)用于保证用户的敏感信息不会在各个平台使用时被泄露出去。Royan是请求为导向的，对每次一个请求只执行一次并不保存任何状态。<strong>threat model:</strong>只信任Ryoan和intel SGX。<strong>Intel SGX:</strong>enclave是被保护的区域，只能被enclave code访问，Ryoan实例就运行在enclave中。通过利用SGX，Ryoan可以保证用户隐私数据在不被信任的平台被使用。<strong>问题：</strong>1.平台可以读用户的私密信息。Sol:enclave可以保护不被外部读数据；2.平台可以将整段encalve中的私密信息复制到不被enclave保护的区域。Sol:nacl限制了sandbox的访问控制。3.利用sys call将私密信息直接写到未被保护的区域。Sol:对sandbox访问的sys<br>call获得的数据都进行加密。4.串通或者伪造用户窃取数据。Sol：royan是不保存任何请求的状态，即每次都会经历初始化-读入数据-处理-输出数据-销毁的过程。<strong>旁路攻击：</strong>解决方法是尽量移除module的system calls，同时保证执行所有输入和输出数据的过程是与数据内容无关的。与第4个问题的解决方法相似，中间有个问题就是初始化阶段overhead很大，优化方法是对初始化阶段做checkpoint这样处理重复请求可以节约时间，但是其他阶段仍然保持无状态，每次都重新执行。同时，一些不可避免的system call则换成功能受限的sys call，如mmap和open、write等“new”sys call则返回预先申请好的安全的内存或者文件。</p>
<h2 id="考试题型"><a href="#考试题型" class="headerlink" title="考试题型"></a>考试题型</h2><p>某个概念是什么</p>
<p>给定一个场景，让你提出解决方案</p>

      
      <!-- 打赏 -->
      
      <div id="reward-btn">
        打赏
      </div>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://dingzhenkai.github.io/2020/06/23/ADS-Notes/" data-id="ckg91iaty0040xzj51naua65x"
        class="article-share-link">分享</a>
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ADS/" rel="tag">ADS</a></li></ul>

    </footer>

  </div>

  
  
  <nav class="article-nav">
    
      <a href="/2020/07/29/IOS-begin/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            IOS_begin
          
        </div>
      </a>
    
    
      <a href="/2020/05/19/docker%E8%BF%81%E7%A7%BB-var-lib-docker/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">docker迁移/var/lib/docker</div>
      </a>
    
  </nav>


  

  
  
<!-- valine评论 -->
<div id="vcomments-box">
    <div id="vcomments">
    </div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src='https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js'></script>
<script>
    new Valine({
        el: '#vcomments',
        notify: false,
        verify: '',
        app_id: '',
        app_key: '',
        path: window.location.pathname,
        avatar: 'mp',
        placeholder: '给我的文章加点评论吧~',
        recordIP: true
    });
    const infoEle = document.querySelector('#vcomments .info');
    if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
        infoEle.childNodes.forEach(function (item) {
            item.parentNode.removeChild(item);
        });
    }
</script>
<style>
    #vcomments-box {
        padding: 5px 30px;
    }

    @media screen and (max-width: 800px) {
        #vcomments-box {
            padding: 5px 0px;
        }
    }

    #vcomments-box #vcomments {
        background-color: #fff;
    }

    .v .vlist .vcard .vh {
        padding-right: 20px;
    }

    .v .vlist .vcard {
        padding-left: 10px;
    }
</style>

  

  
  
  

</article>
</section>
      <footer class="footer">
  <div class="outer">
    <ul class="list-inline">
      <li>
        &copy;
        2020
        Bonjour Ding
      </li>
      <li>
        
          Powered by
        
        
        <a href="https://hexo.io" target="_blank">Hexo</a> Theme <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a>
        
      </li>
    </ul>
    <ul class="list-inline">
      <li>
        
        
        <ul class="list-inline">
  <li>PV:<span id="busuanzi_value_page_pv"></span></li>
  <li>UV:<span id="busuanzi_value_site_uv"></span></li>
</ul>
        
      </li>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>
    <div class="to_top">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>
      </div>
    </main>
      <aside class="sidebar">
        <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/favicon.ico" alt="丁星乐"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/2020/01/31/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
      </aside>
      <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
      
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>



<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js"></script>
<script>
  var typed = new Typed("#subtitle", {
    strings: ['芒焰藏于简单之中','江山如此多娇','会当击水三千里，自信人生二百年'],
    startDelay: 0,
    typeSpeed: 200,
    loop: true,
    backSpeed: 100,
    showCursor: true
    });
</script>




<script src="/js/tocbot.min.js"></script>

<script>
  // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
  tocbot.init({
    tocSelector: '.tocbot',
    contentSelector: '.article-entry',
    headingSelector: 'h1, h2, h3, h4, h5, h6',
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer:'main',
    positionFixedSelector: '.tocbot',
    positionFixedClass: 'is-position-fixed',
    fixedSidebarOffset: 'auto',
    onClick: (e) => {
      $('.toc-link').removeClass('is-active-link');
      $(`a[href=${e.target.hash}]`).addClass('is-active-link');
      $(e.target.hash).scrollIntoView();
      return false;
    }
  });
</script>


<script>
  var ayerConfig = {
    mathjax: false
  }
</script>


<script src="/js/ayer.js"></script>


<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">


<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script>




<script type="text/javascript" src="https://js.users.51.la/20544303.js"></script>
  </div>
</body>

</html>